/**
 * Copyright or © or Copr. IETR/INSA - Rennes (2013 - 2019) :
 *
 * Julien Hascoet <jhascoet@kalray.eu> (2016 - 2017)
 * Karol Desnos <karol.desnos@insa-rennes.fr> (2013 - 2017)
 *
 * This software is a computer program whose purpose is to help prototyping
 * parallel applications using dataflow formalism.
 *
 * This software is governed by the CeCILL  license under French law and
 * abiding by the rules of distribution of free software.  You can  use,
 * modify and/ or redistribute the software under the terms of the CeCILL
 * license as circulated by CEA, CNRS and INRIA at the following URL
 * "http://www.cecill.info".
 *
 * As a counterpart to the access to the source code and  rights to copy,
 * modify and redistribute granted by the license, users are provided only
 * with a limited warranty  and the software's author,  the holder of the
 * economic rights,  and the successive licensors  have only  limited
 * liability.
 *
 * In this respect, the user's attention is drawn to the risks associated
 * with loading,  using,  modifying and/or developing or reproducing the
 * software by the user in light of its specific status of free software,
 * that may mean  that it is complicated to manipulate,  and  that  also
 * therefore means  that it is reserved for developers  and  experienced
 * professionals having in-depth computer knowledge. Users are therefore
 * encouraged to load and test the software's suitability as regards their
 * requirements in conditions enabling the security of their systems and/or
 * data to be ensured and,  more generally, to use and operate it in the
 * same conditions as regards security.
 *
 * The fact that you are presently reading this means that you have had
 * knowledge of the CeCILL license and that you accept its terms.
 */
package org.preesm.codegen.xtend.printer.c

import java.util.Collection
import java.util.Date
import java.util.LinkedHashMap
import java.util.List
import java.util.Map
import org.preesm.codegen.model.Block
import org.preesm.codegen.model.Buffer
import org.preesm.codegen.model.CallBlock
import org.preesm.codegen.model.Communication
import org.preesm.codegen.model.Constant
import org.preesm.codegen.model.ConstantString
import org.preesm.codegen.model.CoreBlock
import org.preesm.codegen.model.Delimiter
import org.preesm.codegen.model.Direction
import org.preesm.codegen.model.FifoCall
import org.preesm.codegen.model.FifoOperation
import org.preesm.codegen.model.FiniteLoopBlock
import org.preesm.codegen.model.FunctionCall
import org.preesm.codegen.model.LoopBlock
import org.preesm.codegen.model.NullBuffer
import org.preesm.codegen.model.SharedMemoryCommunication
import org.preesm.codegen.model.SpecialCall
import org.preesm.codegen.model.SubBuffer
import org.preesm.codegen.model.Variable
import org.preesm.commons.exceptions.PreesmRuntimeException

class MPPA2IOExplicitPrinter extends MPPA2ExplicitPrinter {

	new() {
		// do not generate a main file
		super(true)
	}

	protected int sharedOnly = 1;
	protected String peName = "";
	protected Map<String, Integer> pesToId = new LinkedHashMap<String, Integer>();
	/**
	 * Temporary global var to ignore the automatic suppression of memcpy
	 * whose target and destination are identical.
	 */
	protected boolean IGNORE_USELESS_MEMCPY = true

	protected String local_buffer = "local_buffer"

	protected boolean IS_HIERARCHICAL = false

	protected String scratch_pad_buffer = ""

	protected long local_buffer_size = 0
	
	override printCoreBlockHeader(CoreBlock block)  {
	
	this.peName = block.name;
	
	var String printing = '''
		/**
		 * @file «block.name».c
		 * @generated by «this.class.simpleName»
		 * @date «new Date»
		 */

		/* system includes */
		#include <stdlib.h>
		#include <stdio.h>
		#include <stdint.h>
		#include <mOS_vcore_u.h>
		#include <mppa_noc.h>
		#include <mppa_rpc.h>
		#include <mppa_async.h>
		#include <pthread.h>
		#include <semaphore.h>
		#ifndef __nodeos__
		#include <utask.h>
		#endif

		/* user includes */
		#include "preesm_gen.h"

		extern void *__wrap_memset (void *s, int c, size_t n);
		extern void *__wrap_memcpy(void *dest, const void *src, size_t n);
		
		#define memset __wrap_memset
		#define memcpy __wrap_memcpy
		
		extern mppa_async_segment_t shared_segment;
		extern mppa_async_segment_t distributed_segment[PREESM_NB_CLUSTERS + PREESM_IO_USED];

		/* Scratchpad buffer ptr (will be malloced) */
		char *local_buffer = NULL;
		/* Scratchpad buffer size */
		int local_buffer_size = 0;

	'''
	return printing;
	
	}
	override printBufferDefinition(Buffer buffer) {
		
		if(!buffer.name.equals("Shared")){
			this.sharedOnly = 0;
		}
	
		var result = '''
		«IF buffer.name == "Shared"»
		//#define Shared ((char*)0x10000000ULL) 	/* Shared buffer in DDR */
		«ELSE»
		«buffer.type» «buffer.name»[«buffer.size»] __attribute__ ((aligned(64))); // «buffer.comment» size:= «buffer.size»*«buffer.type» aligned on data cache line
		int local_memory_size = «buffer.size»;
		«ENDIF»
		'''
		return result;
	}

	override printDefinitionsHeader(List<Variable> list) '''
	«IF !list.empty»
		// Core Global Definitions

	«ENDIF»
	'''

	override printSubBufferDefinition(SubBuffer buffer) '''
	«buffer.type» *const «buffer.name» = («buffer.type»*) («var offset = 0L»«
	{offset = buffer.offset
	 var b = buffer.container;
	 while(b instanceof SubBuffer){
	 	offset = offset + b.offset
	  	b = b.container
	  }
	 b}.name»+«offset»);  // «buffer.comment» size:= «buffer.size»*«buffer.type»
	'''

	override printFiniteLoopBlockHeader(FiniteLoopBlock block2) '''
	«{
	 	IS_HIERARCHICAL = true
	"\t"}»// Begin the for loop
	{
		«{
		var gets = ""
		var local_offset = 0L;
			/* go through eventual out param first because of foot FiniteLoopBlock */
			for(param : block2.outBuffers){
				var b = param.container;
				var offset = param.offset;
				while(b instanceof SubBuffer){
					offset += b.offset;
					b = b.container;
				}
				/* put out buffer here */
				if(b.name == "Shared"){
					gets += "void *" + param.name + " = local_buffer+" + local_offset +";\n";
					local_offset += param.typeSize * param.size;
				}
			}
			for(param : block2.inBuffers){
				var b = param.container;
				var offset = param.offset;
				while(b instanceof SubBuffer){
					offset += b.offset;
					b = b.container;
				}
				//System.out.print("===> " + b.name + "\n");
				if(b.name == "Shared"){
					gets += "void *" + param.name + " = local_buffer+" + local_offset +";\n";
					gets += "{\n"
					gets += "	mppa_async_get(local_buffer + " + local_offset + ",\n";
					gets += "	&shared_segment,\n";
					//gets += "	" + b.name + " + " + offset + ",\n";
					gets += "	/* Shared + */ " + offset + ",\n";
					gets += "	" + param.typeSize * param.size + ",\n";
					gets += "	NULL);\n";
					gets += "}\n"
					local_offset += param.typeSize * param.size;
					//System.out.print("==> " + b.name + " " + param.name + " size " + param.size + " port_name "+ port.getName + "\n");
				}
			}

			gets += "int " + block2.iter.name + ";\n"
			gets += "#pragma omp parallel for private(" + block2.iter.name + ")\n"
			gets += "for(" + block2.iter.name + "=0;" + block2.iter.name +"<" + block2.nbIter + ";" + block2.iter.name + "++)\n"
			gets += "	{\n"


			if(local_offset > local_buffer_size)
				local_buffer_size = local_offset
	gets}»
	'''

	override printFiniteLoopBlockFooter(FiniteLoopBlock block2) '''
		}
		«{
				var puts = ""
				var local_offset = 0L;
				for(param : block2.outBuffers){
					var b = param.container
					var offset = param.offset
					while(b instanceof SubBuffer){
						offset += b.offset;
						b = b.container;
						//System.out.print("Running through all buffer " + b.name + "\n");
					}
					//System.out.print("===> " + b.name + "\n");
					if(b.name == "Shared"){
						puts += "{\n"
						puts += "	mppa_async_put(local_buffer + " + local_offset + ",\n";
						puts += "	&shared_segment,\n";
						puts += "	/* Shared + */" + offset + ",\n";
						puts += "	" + param.typeSize * param.size + ",\n";
						puts += "	NULL);\n";
						puts += "	}\n"
						local_offset += param.typeSize * param.size;
						//System.out.print("==> " + b.name + " " + param.name + " size " + param.size + " port_name "+ port.getName + "\n");
					}
				}
				if(local_offset > local_buffer_size)
					local_buffer_size = local_offset
				puts += "}\n"
			puts}»
		«{
			 	IS_HIERARCHICAL = false
			""}»
	'''

	override printFunctionCall(FunctionCall functionCall) '''
	«{
		var gets = ""
		var local_offset = 0L;
		if(IS_HIERARCHICAL == false){
			gets += "{\n"
			for(param : functionCall.parameters){

				if(param instanceof SubBuffer){
					var port = functionCall.parameterDirections.get(functionCall.parameters.indexOf(param))
					var b = param.container;
					var offset = param.offset;
					while(b instanceof SubBuffer){
						offset += b.offset;
						b = b.container;
						//System.out.print("Running through all buffer " + b.name + " --- " + this.peName + "\n");
					}
					//System.out.print("===> " + b.name + "\n");
					if(b.name == "Shared"){
						gets += "	void *" + param.name + " = local_buffer+" + local_offset +";\n";
						if(port.getName == "INPUT"){ /* we get data from DDR -> cluster only when INPUT */
							gets += "	{\n"
							gets += "		mppa_async_get(local_buffer+" + local_offset + ", &shared_segment, /* Shared + */ " + offset + ", " + param.typeSize * param.size + ", NULL);\n";
							gets += "	}\n"
						}
						local_offset += param.typeSize * param.size;
						//System.out.print("==> " + b.name + " " + param.name + " size " + param.size + " port_name "+ port.getName + "\n");
					} 
					/*else{
						System.out.print("A==> " + b.name + " " + param.name + " size " + param.size + " port_name "+ port.getName + "\n");
					}*/
				}
			}
			gets += "\t"
		}else{
			gets += " /* gets are normaly generated before */ \n"
		}
		if(local_offset > local_buffer_size)
			local_buffer_size = local_offset
	gets}»
		«functionCall.name»(«FOR param : functionCall.parameters SEPARATOR ', '»«param.doSwitch»«ENDFOR»); // «functionCall.actorName»
	«{
		var puts = ""
		var local_offset = 0L;
		if(IS_HIERARCHICAL == false){
			for(param : functionCall.parameters){
				if(param instanceof SubBuffer){
					var port = functionCall.parameterDirections.get(functionCall.parameters.indexOf(param))
					var b = param.container
					var offset = param.offset
					while(b instanceof SubBuffer){
						offset += b.offset;
						b = b.container;
						//System.out.print("Running through all buffer " + b.name + "\n");
					}
					//System.out.print("===> " + b.name + "\n");
					if(b.name == "Shared"){
						if(port.getName == "OUTPUT"){ /* we put data from cluster -> DDR only when OUTPUT */
							puts += "	{\n"
							puts += "		mppa_async_put(local_buffer+" + local_offset + ", &shared_segment, /* Shared + */ " + offset + ", " + param.typeSize * param.size + ", NULL);\n";
							puts += "	}\n"
						}
						local_offset += param.typeSize * param.size;
						//System.out.print("==> " + b.name + " " + param.name + " size " + param.size + " port_name "+ port.getName + "\n");
					}			
					/*else{
						System.out.print("B==> " + b.name + " " + param.name + " size " + param.size + " port_name "+ port.getName + "\n");
					}*/
				}
			}
			puts += "}\n"
		}else{
			puts += " /* puts are normaly generated before */ \n"
		}
		if(local_offset > local_buffer_size)
			local_buffer_size = local_offset
	puts}»
	'''

	override printDefinitionsFooter(List<Variable> list) '''
	«IF !list.empty»

	«ENDIF»
	'''

	override printDeclarationsHeader(List<Variable> list) '''
	// Core Global Declaration
	extern pthread_barrier_t iter_barrier;
	extern int stopThreads;

	'''

	override printBufferDeclaration(Buffer buffer) '''
	extern «printBufferDefinition(buffer)»
	'''

	override printSubBufferDeclaration(SubBuffer buffer) {
		/*var bChecker = buffer.container;
		while(bChecker instanceof SubBuffer){
	  		bChecker = bChecker.container
	  	}*/
	  	var printing = "";
	  //	if(bChecker.name.equals("Shared")){
	  		printing = '''
			«buffer.type» *const «buffer.name» = («buffer.type»*) («var offset = 0L»«
			{offset = buffer.offset
			 var b = buffer.container;
			 while(b instanceof SubBuffer){
			 	offset = offset + b.offset
			  	b = b.container
			  }
			 b}.name»+«offset»);  // «buffer.comment» size:= «buffer.size»*«buffer.type»
			'''
	  	//}
	
	return printing;
	}

	override printDeclarationsFooter(List<Variable> list) '''
	«IF !list.empty»

	«ENDIF»
	'''

	override printCoreInitBlockHeader(CallBlock callBlock) '''
	void *computationTask_«(callBlock.eContainer as CoreBlock).name»(void *arg __attribute__ ((unused))){
«/*	#ifdef PREESM_VERBOSE
		//printf("Cluster %d runs on task «(callBlock.eContainer as CoreBlock).name»\n", __k1_get_cluster_id());
	#endif*/»
		«IF !callBlock.codeElts.empty»
			// Initialisation(s)

		«ENDIF»
	'''

	override printCoreLoopBlockHeader(LoopBlock block2) '''

		«"\t"»// Begin the execution loop 
		#ifdef PREESM_LOOP_SIZE // Case of a finite loop
			int __iii;
			for(__iii=0;__iii<PREESM_LOOP_SIZE;__iii++){
		#else // Default case of an infinite loop
			while(!stopThreads){
		#endif

				//pthread_barrier_wait(&iter_barrier);

	'''


	override printCoreLoopBlockFooter(LoopBlock block2) '''

				/* commit local changes to the global memory */
				//pthread_barrier_wait(&iter_barrier); /* barrier to make sure all threads have commited data in smem */
			}
			return NULL;
		}
	'''
	override printFifoCall(FifoCall fifoCall) {
		var result = "fifo" + fifoCall.operation.toString.toLowerCase.toFirstUpper + "("

		if (fifoCall.operation != FifoOperation::INIT) {
			var buffer = fifoCall.parameters.head as Buffer
			result = result + '''«buffer.doSwitch», '''
		}

		result = result +
			'''«fifoCall.headBuffer.name», «fifoCall.headBuffer.size»*sizeof(«fifoCall.headBuffer.type»), '''
		result = result + '''«IF fifoCall.bodyBuffer !== null»«fifoCall.bodyBuffer.name», «fifoCall.bodyBuffer.size»*sizeof(«fifoCall.
			bodyBuffer.type»)«ELSE»NULL, 0«ENDIF»);
			'''

		return result
	}

	override printFork(SpecialCall call) '''
	// Fork «call.name»«var input = call.inputBuffers.head»«var index = 0L»
	{
		«FOR output : call.outputBuffers»
			«printMemcpy(output,0,input,index,output.size,output.type)»«{index=(output.size+index); ""}»
		«ENDFOR»
	}
	'''

	override printBroadcast(SpecialCall call) '''
		«{
			super.printBroadcast(call)
		}»
	'''

	override printRoundBuffer(SpecialCall call) '''
		«{
			super.printRoundBuffer(call)
		}»
	'''

	override printJoin(SpecialCall call) '''
	// Join «call.name»«var output = call.outputBuffers.head»«var index = 0L»
	{
		«FOR input : call.inputBuffers»
			«printMemcpy(output,index,input,0,input.size,input.type)»«{index=(input.size+index); ""}»
		«ENDFOR»
	}
	'''

	/**
	 * Print a memcpy call in the generated code. Unless
	 * {@link #IGNORE_USELESS_MEMCPY} is set to <code>true</code>, this method
	 * checks if the destination and the source of the memcpy are superimposed.
	 * In such case, the memcpy is useless and nothing is printed.
	 *
	 * @param output
	 *            the destination {@link Buffer}
	 * @param outOffset
	 *            the offset in the destination {@link Buffer}
	 * @param input
	 *            the source {@link Buffer}
	 * @param inOffset
	 *            the offset in the source {@link Buffer}
	 * @param size
	 *            the amount of memory to copy
	 * @param type
	 *            the type of objects copied
	 * @return a {@link CharSequence} containing the memcpy call (if any)
	 */
	override printMemcpy(Buffer output, long outOffset, Buffer input, long inOffset, long size, String type) {

		// Retrieve the container buffer of the input and output as well
		// as their offset in this buffer
		var totalOffsetOut = outOffset*output.typeSize
		var bOutput = output
		while (bOutput instanceof SubBuffer) {
			totalOffsetOut = totalOffsetOut + bOutput.offset
			bOutput = bOutput.container
		}

		var totalOffsetIn = inOffset*input.typeSize
		var bInput = input
		while (bInput instanceof SubBuffer) {
			totalOffsetIn = totalOffsetIn + bInput.offset
			bInput = bInput.container
		}

		// If the Buffer and offsets are identical, or one buffer is null
		// there is nothing to print
		if((IGNORE_USELESS_MEMCPY && bInput == bOutput && totalOffsetIn == totalOffsetOut) ||
			output instanceof NullBuffer || input instanceof NullBuffer){
			return ""
		} else {
			return '''memcpy(«output.doSwitch»+«outOffset», «input.doSwitch»+«inOffset», «size»*sizeof(«type»));'''
		}
	}

	override printNullBuffer(NullBuffer Buffer) {
		return printBuffer(Buffer)
	}

	override caseCommunication(Communication communication) {

		if(communication.nodes.forall[type == "SHARED_MEM"]) {
			return super.caseCommunication(communication)
		} else {
			throw new PreesmRuntimeException("Communication "+ communication.name +
				 " has at least one unsupported communication node"+
				 " for the " + this.class.name + " printer")
		}
	}

	override printSharedMemoryCommunication(SharedMemoryCommunication communication) '''
		«communication.direction.toString.toLowerCase»«communication.delimiter.toString.toLowerCase.toFirstUpper»(«IF (communication.
			direction == Direction::SEND && communication.delimiter == Delimiter::START) ||
			(communication.direction == Direction::RECEIVE && communication.delimiter == Delimiter::END)»«{
			var coreID = if (communication.direction == Direction::SEND) {
					communication.receiveStart.coreContainer.coreID
				} else {
					communication.sendStart.coreContainer.coreID
				}
			var ret = coreID
			ret
		}»«ENDIF»); // «communication.sendStart.coreContainer.name» > «communication.receiveStart.coreContainer.name»: «communication.
			data.doSwitch»
	'''

	override printConstant(Constant constant) '''«constant.value»«IF !constant.name.nullOrEmpty»/*«constant.name»*/«ENDIF»'''

	override printConstantString(ConstantString constant) '''"«constant.value»"'''

	override printBuffer(Buffer buffer) '''«buffer.name»'''

	override printSubBuffer(SubBuffer buffer) {
		return printBuffer(buffer)
	}
	
	override createSecondaryFiles(List<Block> printerBlocks, Collection<Block> allBlocks) {
		val result = super.createSecondaryFiles(printerBlocks, allBlocks);
		result.remove("cluster_main.c");
		result.remove("host_main.c");
		if (generateMainFile()) {
			result.put("io_main.c", printMainIO(printerBlocks));
		}
		return result
	}

	override String printMainIO(List<Block> printerBlocks) '''
		/**
		 * @file io_main.c
		 * @generated by «this.class.simpleName»
		 * @date «new Date»
		 *
		 */
		/*
		 * Copyright (C) 2016 Kalray SA.
		 *
		 * All rights reserved.
		 */
		
		#include <stdio.h>
		#include <stdlib.h>
		#include "mppa_boot_args.h"
		#include <mppa_power.h>
		#include <assert.h>
		#include "mppa_bsp.h"
		#include <utask.h>
		#include <pcie_queue.h>
		#include <mppa_rpc.h>
		#include <mppa_remote.h>
		#include <mppa_async.h>
		#include <HAL/hal/board/boot_args.h>
		#include "preesm_gen.h"
		#include "communication.h"
		
		/* Shared Segment ID */
		mppa_async_segment_t shared_segment;
		«IF (this.sharedOnly == 0)»
		mppa_async_segment_t distributed_segment[PREESM_NB_CLUSTERS + PREESM_IO_USED];
		extern int local_memory_size;
		«ENDIF»
				
		static utask_t t;
		static mppadesc_t pcie_fd = 0;
		/* extern reference of generated code */
		«FOR io : printerBlocks.toSet»
			«IF (io instanceof CoreBlock)»
				extern void *computationTask_«io.name»(void *arg);
			«ENDIF»
		«ENDFOR»
		/* extern reference of shared memories */ 
		«FOR io : printerBlocks.toSet»
			«IF (io instanceof CoreBlock)»
				extern char *«io.name»;
			«ENDIF»					
		«ENDFOR»
		
		int
		main(int argc __attribute__ ((unused)), char *argv[] __attribute__ ((unused)))
		{
			int id;
			int j;
			int ret ;
		
			if(__k1_spawn_type() == __MPPA_PCI_SPAWN){
				#if 1
				long long *ptr = (void*)(uintptr_t)Shared;
				long long i;
				for(i=0;i<(long long)((1<<30ULL)/sizeof(long long));i++)
				{
					ptr[i] = -1LL;
				}
				__builtin_k1_wpurge();
				__builtin_k1_fence();
				mOS_dinval();
				#endif
			}
		
			if (__k1_spawn_type() == __MPPA_PCI_SPAWN) {
				pcie_fd = pcie_open(0);
					ret = pcie_queue_init(pcie_fd);
					assert(ret == 0);
					pcie_register_console(pcie_fd, stdin, stdout);
			}
					
			if(mppa_rpc_server_init(	1 /* rm where to run server */, 
									0 /* offset ddr */, 
									PREESM_NB_CLUSTERS /* nb_cluster to serve*/) != 0){
				assert(0 && "mppa_rpc_server_init\n");
			}
			if(mppa_async_server_init() != 0){
				assert(0 && "mppa_async_server_init\n");
			}
			if(mppa_remote_server_init(pcie_fd, PREESM_NB_CLUSTERS) != 0){
				assert(0 && "mppa_remote_server_init\n");
			}
			
			if(utask_create(&t, NULL, (void*)mppa_rpc_server_start, NULL) != 0){
				assert(0 && "utask_create\n");
			}
						
			for( j = 0 ; j < PREESM_NB_CLUSTERS ; j++ ) {
		
				char elf_name[30];
				sprintf(elf_name, "cluster%d_bin", j);
				id = mppa_power_base_spawn(j, elf_name, NULL, NULL, MPPA_POWER_SHUFFLING_ENABLED);
				if (id < 0)
					return -2;
			}				
		
			mppa_async_segment_t shared_segment;
			if(mppa_async_segment_create(&shared_segment, SHARED_SEGMENT_ID, (void*)(uintptr_t)Shared, 1024*1024*1024, 0, 0, NULL) != 0)
				assert(0 && "mppa_async_segment_create\n");
			}
			«IF (this.sharedOnly == 0)»
				«FOR io : printerBlocks.toSet»
					«IF (io instanceof CoreBlock)»
						if(mppa_async_segment_create(&distributed_segment[PREESM_NB_CLUSTERS], INTERCC_BASE_SEGMENT_ID+PREESM_NB_CLUSTERS, (void*)(uintptr_t)«io.name», local_memory_size, 0, 0, NULL) != 0){
							assert(0 && "mppa_async_segment_create\n");
						}
					«ENDIF»
				«ENDFOR»	
			«ENDIF»	
			// init comm
			communicationInit();	
			mppa_rpc_barrier(1, 2);
			«IF (this.sharedOnly == 0)»
				int i;
				for(i = 0; i < PREESM_NB_CLUSTERS; i++){
					if(mppa_async_segment_clone(&distributed_segment[i], INTERCC_BASE_SEGMENT_ID+i, NULL, 0, NULL) != 0){
						assert(0 && "mppa_async_segment_clone\n");
					}
				}
				mppa_rpc_barrier(1, 2);
			«ENDIF»
			computationTask_IO(NULL);
			int err;
			for( j = 0 ; j < PREESM_NB_CLUSTERS ; j++ ) {
			    mppa_power_base_waitpid (j, &err, 0);
			}
		
			if (__k1_spawn_type() == __MPPA_PCI_SPAWN) {
				pcie_queue_barrier(pcie_fd, 0, &ret);
				pcie_queue_exit(pcie_fd, ret, NULL);
			}
			return 0;
		}

	'''
	
	override preProcessing(List<Block> printerBlocks, Collection<Block> allBlocks){
		for (cluster : allBlocks){
			if (cluster instanceof CoreBlock) {
				if(cluster.coreType.equals("MPPA2Explicit")){
					numClusters = numClusters + 1;
					clusterToSync = cluster.coreID;
				}
				else if(cluster.coreType.equals("MPPA2IOExplicit")){
					io_used = 1;
				}
				this.pesToId.put(cluster.name, cluster.coreID);
			}				
		}	
		local_buffer_size = 0;
	}
	override postProcessing(CharSequence charSequence){
		var ret = charSequence.toString.replace("int local_buffer_size = 0;", "int local_buffer_size = " + local_buffer_size + ";");
		return ret;
	}
}
